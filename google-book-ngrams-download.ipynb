{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b65d5f03-ebba-424c-a452-8b5351c017f6",
   "metadata": {},
   "source": [
    "Pull Bigrams from the _Google Million_ data set.\n",
    "\n",
    "> The \"Google Million\". All are in English with dates ranging from 1500 to 2008. No more than about 6000 books were chosen from any one year, which means that all of the scanned books from early years are present, and books from later years are randomly sampled. The random samplings reflect the subject distributions for the year (so there are more computer books in 2000 than 1980).\n",
    "\n",
    "See\n",
    "- https://books.google.com/ngrams/info\n",
    "- https://storage.googleapis.com/books/ngrams/books/datasetsv2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976eb9e-6f92-4330-919e-f920f094db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop records from before this year\n",
    "MIN_YEAR = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2578732f-0584-4abd-b192-96d4550250ef",
   "metadata": {},
   "source": [
    "## Step 1: Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70476313-018b-49af-84f8-7682e6d84468",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "815fd56f-da9d-4dc1-9e96-7de03771a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    file = f'googlebooks-eng-1M-2gram-20090715-{i}.csv.zip'\n",
    "    url = f'http://storage.googleapis.com/books/ngrams/books/{file}'\n",
    "    response = rq.get(url)\n",
    "    with open(f'ngrams/{file}', 'wb') as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6b726-a496-4c2d-b066-1d4335bca016",
   "metadata": {},
   "source": [
    "## Step 2: Filter/aggregate\n",
    "\n",
    "Here, we do light filtering and aggregation.\n",
    "\n",
    "For efficiency we drop all records from years prior to `MIN_YEAR`.\n",
    "\n",
    "Because the data is unique at the yearly level, we group by and sum all occurrences at the bigram level\n",
    "over the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f40e6609-2c15-4a88-b983-de8d29e2d925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a757663455794865885bc519821f66ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    file = f'googlebooks-eng-1M-2gram-20090715-{i}.csv.zip'\n",
    "    try:\n",
    "        df = pd.read_csv(f'ngrams/{file}', sep='\\t')\n",
    "    except Exception:\n",
    "        continue\n",
    "    df.columns =['ngram', 'year', 0'occurrences', 'pages', 'books']\n",
    "    df = df[\n",
    "        df.year.ge(MIN_YEAR)\n",
    "        & ~df.ngram.str.contains(f'[^{string.ascii_letters + \" \"}]+')\n",
    "    ].groupby('ngram').occurrences.sum()\n",
    "    df.to_csv(f\"ngrams/{file.replace('.csv.zip', '-clean.csv.gz')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
